{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas import DataFrame as DF\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.cross_validation import cross_val_score as cv\n",
    "train = pd.read_csv('f_train_20180204.csv',encoding='gbk')\n",
    "test = pd.read_csv('f_test_a_20180204.csv',encoding='gbk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train['id']\n",
    "del test['id']\n",
    "feature_name = [i for i in train.columns if i!='label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(nums,cv_fold):\n",
    "    feature_name1 = train_data[feature_name].columns\n",
    "    get_ans_face = list(set(get_pic(gbc_model,feature_name1).head(nums)['name'])&set(get_pic(xgb_model,feature_name1).head(nums)['name'])&set(get_pic(lgb_model,feature_name1).head(nums)['name']))\n",
    "    print('New Feature: ',len(get_ans_face))\n",
    "    if 'SNP32*SNP34' not in get_ans_face:\n",
    "        get_ans_face.append('SNP32*SNP34')\n",
    "    print('New Feature: ',len(get_ans_face))\n",
    "    new_lgb_model = lgb.LGBMClassifier(objective='binary',n_estimators=300,max_depth=3,min_child_samples=6,learning_rate=0.102,random_state=1)\n",
    "    cv_model = cv(new_lgb_model, train_data[get_ans_face], train_label,  cv=cv_fold, scoring='f1')\n",
    "    new_lgb_model.fit(train_data[get_ans_face], train_label)\n",
    "    m1 = cv_model.mean()\n",
    "\n",
    "    new_xgb_model1 = xgb.XGBClassifier(objective='binary:logistic',n_estimators=300,max_depth=4,learning_rate=0.101,random_state=1)\n",
    "    cv_model = cv(new_xgb_model1, train_data[get_ans_face].values, train_label,  cv=cv_fold, scoring='f1')\n",
    "    new_xgb_model1.fit(train_data[get_ans_face].values, train_label)\n",
    "    m2 = cv_model.mean()\n",
    "\n",
    "    new_gbc_model = GBC(n_estimators=310,subsample=1,min_samples_split=2,max_depth=3,learning_rate=0.1900,min_weight_fraction_leaf=0.1)\n",
    "    kkk = train_data[get_ans_face].fillna(7)\n",
    "    cv_model = cv(new_gbc_model, kkk[get_ans_face], train_label,  cv=cv_fold, scoring='f1')\n",
    "    new_gbc_model.fit(kkk.fillna(7),train_label)\n",
    "\n",
    "    m3 = cv_model.mean()\n",
    "    print((m1+m2+m3)/3)\n",
    "    pro1 = new_lgb_model.predict_proba(test_data[get_ans_face])\n",
    "    pro2 = new_xgb_model1.predict_proba(test_data[get_ans_face].values)\n",
    "    pro3 = new_gbc_model.predict_proba(test_data[get_ans_face].fillna(7).values)\n",
    "    ans = (pro1+pro2+pro3)/3\n",
    "    return ans\n",
    "    \n",
    "# temp = [140,160,180,200,220,240,260,280,300,320]\n",
    "\n",
    "# ans = []\n",
    "# for i in range(len(temp)):\n",
    "#     print('Now All Feature:',temp[i])\n",
    "#     ans = get_model(temp[i],5)\n",
    "#     if i == 0:\n",
    "#         ans1 = ans\n",
    "#     else:\n",
    "#         ans1 += ans\n",
    "# ans1 /= len(temp)\n",
    "\n",
    "def find_best_feature(feature_name,cv_fold):\n",
    "    get_ans_face = feature_name\n",
    "    new_lgb_model = lgb.LGBMClassifier(objective='binary',n_estimators=300,max_depth=3,min_child_samples=6,learning_rate=0.102,random_state=1)\n",
    "    cv_model = cv(new_lgb_model, train_data[get_ans_face], train_label,  cv=cv_fold, scoring='f1')\n",
    "    new_lgb_model.fit(train_data[get_ans_face], train_label)\n",
    "    m1 = cv_model.mean()\n",
    "\n",
    "    new_xgb_model1 = xgb.XGBClassifier(objective='binary:logistic',n_estimators=300,max_depth=4,learning_rate=0.101,random_state=1)\n",
    "    cv_model = cv(new_xgb_model1, train_data[get_ans_face].values, train_label,  cv=cv_fold, scoring='f1')\n",
    "    new_xgb_model1.fit(train_data[get_ans_face].values, train_label)\n",
    "    m2 = cv_model.mean()\n",
    "\n",
    "    new_gbc_model = GBC(n_estimators=310,subsample=1,min_samples_split=2,max_depth=3,learning_rate=0.1900,min_weight_fraction_leaf=0.1)\n",
    "    kkk = train_data[get_ans_face].fillna(7)\n",
    "    cv_model = cv(new_gbc_model, kkk[get_ans_face], train_label,  cv=cv_fold, scoring='f1')\n",
    "    new_gbc_model.fit(kkk.fillna(7),train_label)\n",
    "    m3 = cv_model.mean()\n",
    "    return (m1+m2+m3)/3\n",
    "\n",
    "def train_best_feature(feature_name):\n",
    "    get_ans_face = feature_name\n",
    "    new_lgb_model = lgb.LGBMClassifier(objective='binary',n_estimators=300,max_depth=3,min_child_samples=6,learning_rate=0.102,random_state=1)\n",
    "    new_lgb_model.fit(train_data[get_ans_face], train_label)\n",
    "\n",
    "    new_xgb_model1 = xgb.XGBClassifier(objective='binary:logistic',n_estimators=300,max_depth=4,learning_rate=0.101,random_state=1)\n",
    "    new_xgb_model1.fit(train_data[get_ans_face].values, train_label)\n",
    "\n",
    "    new_gbc_model = GBC(n_estimators=310,subsample=1,min_samples_split=2,max_depth=3,learning_rate=0.1900,min_weight_fraction_leaf=0.1)\n",
    "    kkk = train_data[get_ans_face].fillna(7)\n",
    "    new_gbc_model.fit(kkk.fillna(7),train_label)\n",
    "    \n",
    "    pro1 = new_lgb_model.predict_proba(test_data[get_ans_face])\n",
    "    pro2 = new_xgb_model1.predict_proba(test_data[get_ans_face].values)\n",
    "    pro3 = new_gbc_model.predict_proba(test_data[get_ans_face].fillna(7).values)\n",
    "    ans = (pro1+pro2+pro3)/3\n",
    "\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat([train],axis=0)\n",
    "train_label = train_data['label']\n",
    "del train_data['label']\n",
    "test_data = test[feature_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_SNP = [i for i in feature_name if 'SNP' in i]\n",
    "feature_no_SNP = list(set(feature_name)-set(feature_SNP))\n",
    "train_no_SNP_mean = train.describe().T[['mean','min','max']].T[feature_no_SNP]\n",
    "train_no_SNP = train[feature_no_SNP]\n",
    "train_SNP = train[feature_SNP]\n",
    "test_no_SNP_mean = test.describe().T[['mean','min','max']].T[feature_no_SNP]\n",
    "test_SNP = test[feature_SNP]\n",
    "test_no_SNP = test[feature_no_SNP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_SNP.to_csv('train_no_SNP.csv',index=False)\n",
    "test_no_SNP.to_csv('test_no_SNP.csv',index=False)\n",
    "train_SNP.to_csv('train_SNP.csv',index=False)\n",
    "test_SNP.to_csv('test_SNP.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_division_feature(data,feature_name):\n",
    "    new_feature = []\n",
    "    new_feature_name = []\n",
    "    for i in range(len(data[feature_name].columns)-1):\n",
    "        for j in range(i+1,len(data[feature_name].columns)):\n",
    "            new_feature_name.append(data[feature_name].columns[i] + '/' + data[feature_name].columns[j])\n",
    "            new_feature_name.append(data[feature_name].columns[i] + '*' + data[feature_name].columns[j])\n",
    "            new_feature_name.append(data[feature_name].columns[i] + '+' + data[feature_name].columns[j])\n",
    "            new_feature_name.append(data[feature_name].columns[i] + '-' + data[feature_name].columns[j])\n",
    "            new_feature.append(data[data[feature_name].columns[i]]/data[data[feature_name].columns[j]])\n",
    "            new_feature.append(data[data[feature_name].columns[i]]*data[data[feature_name].columns[j]])\n",
    "            new_feature.append(data[data[feature_name].columns[i]]+data[data[feature_name].columns[j]])\n",
    "            new_feature.append(data[data[feature_name].columns[i]]-data[data[feature_name].columns[j]])\n",
    "            \n",
    "    \n",
    "    temp_data = DF(pd.concat(new_feature,axis=1))\n",
    "    temp_data.columns = new_feature_name\n",
    "    data = pd.concat([data,temp_data],axis=1).reset_index(drop=True)\n",
    "    \n",
    "    print(data.shape)\n",
    "    \n",
    "    return data.reset_index(drop=True)\n",
    "\n",
    "def get_square_feature(data,feature_name):\n",
    "    new_feature = []\n",
    "    new_feature_name = []\n",
    "    for i in range(len(data[feature_name].columns)):\n",
    "        new_feature_name.append(data[feature_name].columns[i] + '**2')\n",
    "        new_feature_name.append(data[feature_name].columns[i] + '**1/2')\n",
    "        new_feature.append(data[data[feature_name].columns[i]]**2)\n",
    "        new_feature.append(data[data[feature_name].columns[i]]**(1/2))\n",
    "        \n",
    "    temp_data = DF(pd.concat(new_feature,axis=1))\n",
    "    temp_data.columns = new_feature_name\n",
    "    data = pd.concat([data,temp_data],axis=1).reset_index(drop=True)\n",
    "    \n",
    "    print(data.shape)\n",
    "    \n",
    "    return data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 56)\n",
      "(200, 56)\n",
      "(1000, 5995)\n",
      "(1000, 1540)\n",
      "(200, 5995)\n",
      "(200, 1540)\n",
      "7591\n",
      "(1000, 7591)\n",
      "(200, 7591)\n"
     ]
    }
   ],
   "source": [
    "train_data = get_square_feature(train_no_SNP,feature_no_SNP)\n",
    "test_data = get_square_feature(test_no_SNP,feature_no_SNP)\n",
    "\n",
    "train_data_SNP = get_division_feature(train_SNP,train_SNP.columns)\n",
    "train_data_no_SNP = get_division_feature(train_no_SNP,train_no_SNP.columns)\n",
    "train_data = pd.concat([train_data_SNP,train_data_no_SNP,train_data],axis=1)\n",
    "test_data_SNP = get_division_feature(test_SNP,test_SNP.columns)\n",
    "test_data_no_SNP = get_division_feature(test_no_SNP,test_no_SNP.columns)\n",
    "test_data = pd.concat([test_data_SNP,test_data_no_SNP,test_data],axis=1)\n",
    "\n",
    "feature_name = [i for i in train_data.columns if i!='label']\n",
    "print(len(train_data.columns))\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', colsample_bytree=1.0, learning_rate=0.1,\n",
       "        max_bin=255, max_depth=-1, min_child_samples=20,\n",
       "        min_child_weight=0.001, min_split_gain=0.0, n_estimators=120,\n",
       "        n_jobs=-1, nthread=4, num_leaves=31, objective='binary',\n",
       "        random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "        subsample=0.9, subsample_for_bin=200000, subsample_freq=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_model = lgb.LGBMClassifier(objective='binary',n_estimators=120,subsample=0.9,nthread=4)\n",
    "# cv_model = cv(lgb_model, train_data[feature_name], train_label,  cv=10, scoring='f1')\n",
    "lgb_model.fit(train_data[feature_name], train_label)\n",
    "# print(cv_model)\n",
    "# print(cv_model.mean())\n",
    "\n",
    "# mean 0.650 166 feature\n",
    "# mean 0.650 6900 feature\n",
    "# median 0.648"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pic(model,feature_name):\n",
    "    ans = DF()\n",
    "    ans['name'] = feature_name\n",
    "    ans['score'] = model.feature_importances_\n",
    "#     print(ans[ans['score']>0].shape)\n",
    "    return ans.sort_values(by=['score'],ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=120,\n",
       "       n_jobs=1, nthread=4, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=0.9)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic',n_estimators=120,subsample=0.9,nthread=4)\n",
    "# cv_model = cv(xgb_model, train_data[feature_name].values, train_label,  cv=10, scoring='f1')\n",
    "xgb_model.fit(train_data[feature_name].values, train_label)\n",
    "# print(cv_model)\n",
    "# print(cv_model.mean())\n",
    "\n",
    "# mean 166 632\n",
    "# median 0.657"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "              presort='auto', random_state=None, subsample=0.9, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc_model = GBC(n_estimators=200,subsample=0.9,min_samples_split=2)\n",
    "kkk = train_data[feature_name].fillna(7)\n",
    "kkk.replace(np.inf,999,inplace=True)\n",
    "# cv_model = cv(gbc_model, kkk[feature_name], train_label,  1cv=10, scoring='f1')\n",
    "gbc_model.fit(kkk.fillna(7),train_label)\n",
    "# print(cv_model)\n",
    "# print(cv_model.mean())\n",
    "\n",
    "# mean 0.653\n",
    "# median 0.664"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7591"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Feature:  96\n"
     ]
    }
   ],
   "source": [
    "nums = 45\n",
    "feature_name1 = train_data[feature_name].columns\n",
    "get_ans_face = list(set(get_pic(lgb_model,feature_name1).head(nums)['name'])|set(get_pic(xgb_model,feature_name1).head(nums)['name'])|set(get_pic(gbc_model,feature_name1).head(nums)['name']))\n",
    "print('New Feature: ',len(get_ans_face))\n",
    "\n",
    "# 320 0.739"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在nums = 400的时候 能够达到0.739"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目前特征长度为 1  目前帅气的cv值是 0.386775127677  成功加入第 1 个 增值为 0.386775127677\n",
      "目前特征长度为 2  目前帅气的cv值是 0.51616384038  成功加入第 2 个 增值为 0.129388712703\n",
      "目前特征长度为 3  目前帅气的cv值是 0.527535265985  成功加入第 3 个 增值为 0.011371425605\n",
      "目前特征长度为 4  目前帅气的cv值是 0.563174983085  成功加入第 4 个 增值为 0.0356397171\n",
      "目前特征长度为 5  目前帅气的cv值是 0.57436190063  成功加入第 5 个 增值为 0.0111869175454\n",
      "目前特征长度为 6  目前帅气的cv值是 0.586587422568  成功加入第 7 个 增值为 0.0122255219373\n",
      "目前特征长度为 7  目前帅气的cv值是 0.593785226558  成功加入第 12 个 增值为 0.00719780399015\n",
      "目前特征长度为 8  目前帅气的cv值是 0.608606465091  成功加入第 14 个 增值为 0.0148212385332\n",
      "目前特征长度为 9  目前帅气的cv值是 0.609209748232  成功加入第 17 个 增值为 0.000603283141013\n",
      "目前特征长度为 10  目前帅气的cv值是 0.620925798111  成功加入第 18 个 增值为 0.011716049879\n",
      "目前特征长度为 11  目前帅气的cv值是 0.634570115268  成功加入第 19 个 增值为 0.0136443171573\n",
      "目前特征长度为 12  目前帅气的cv值是 0.688309863978  成功加入第 20 个 增值为 0.0537397487097\n",
      "目前特征长度为 13  目前帅气的cv值是 0.689758693609  成功加入第 21 个 增值为 0.00144882963117\n",
      "目前特征长度为 14  目前帅气的cv值是 0.692031700018  成功加入第 22 个 增值为 0.00227300640844\n",
      "目前特征长度为 15  目前帅气的cv值是 0.70464125809  成功加入第 24 个 增值为 0.0126095580718\n",
      "目前特征长度为 16  目前帅气的cv值是 0.707376667537  成功加入第 25 个 增值为 0.00273540944779\n",
      "目前特征长度为 17  目前帅气的cv值是 0.707770917495  成功加入第 27 个 增值为 0.000394249957276\n",
      "目前特征长度为 18  目前帅气的cv值是 0.71005231562  成功加入第 28 个 增值为 0.00228139812537\n",
      "目前特征长度为 19  目前帅气的cv值是 0.712136621888  成功加入第 31 个 增值为 0.00208430626829\n",
      "目前特征长度为 20  目前帅气的cv值是 0.718013110585  成功加入第 32 个 增值为 0.00587648869632\n",
      "目前特征长度为 21  目前帅气的cv值是 0.718307792721  成功加入第 37 个 增值为 0.000294682136085\n",
      "目前特征长度为 22  目前帅气的cv值是 0.719082461863  成功加入第 38 个 增值为 0.000774669142242\n",
      "目前特征长度为 23  目前帅气的cv值是 0.721935152094  成功加入第 41 个 增值为 0.00285269023075\n",
      "目前特征长度为 24  目前帅气的cv值是 0.725049329819  成功加入第 44 个 增值为 0.00311417772499\n",
      "目前特征长度为 25  目前帅气的cv值是 0.72606671688  成功加入第 51 个 增值为 0.00101738706101\n",
      "目前特征长度为 26  目前帅气的cv值是 0.729606229912  成功加入第 53 个 增值为 0.00353951303223\n",
      "目前特征长度为 27  目前帅气的cv值是 0.729661495167  成功加入第 61 个 增值为 5.52652553621e-05\n",
      "目前特征长度为 28  目前帅气的cv值是 0.730213956901  成功加入第 62 个 增值为 0.000552461733845\n",
      "目前特征长度为 29  目前帅气的cv值是 0.734158746716  成功加入第 66 个 增值为 0.00394478981494\n"
     ]
    }
   ],
   "source": [
    "now_feature = []\n",
    "check = 0\n",
    "for i in range(len(get_ans_face)):\n",
    "    now_feature.append(get_ans_face[i])\n",
    "    jj = find_best_feature(now_feature,6)\n",
    "    if jj>check:\n",
    "        print('目前特征长度为',len(now_feature),' 目前帅气的cv值是',jj,' 成功加入第',i+1,'个','增值为',jj-check)\n",
    "        check = jj\n",
    "    else:\n",
    "        now_feature.pop()\n",
    "#         print('目前特征长度为',len(now_feature),'第',i+1,'个拉闸了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VAR00007*DM家族史',\n",
       " 'AST-hsCRP',\n",
       " '分娩时/wbc',\n",
       " 'wbc-LDLC',\n",
       " 'SNP32*SNP33',\n",
       " 'SNP20*SNP34',\n",
       " 'ApoB/BUN',\n",
       " 'SNP37/SNP53',\n",
       " 'SNP22/SNP34',\n",
       " 'VAR00007',\n",
       " 'hsCRP+年龄',\n",
       " '年龄+LDLC',\n",
       " 'VAR00007*年龄',\n",
       " 'SNP39*SNP47',\n",
       " 'hsCRP-LDLC',\n",
       " 'TG*年龄',\n",
       " '孕次/Lpa',\n",
       " 'SNP46/SNP47',\n",
       " 'SNP26*SNP48',\n",
       " 'wbc-年龄',\n",
       " '孕前BMI/Cr',\n",
       " 'VAR00007*糖筛孕周',\n",
       " 'SNP16/SNP34',\n",
       " '舒张压/ApoA1',\n",
       " 'BUN/DM家族史',\n",
       " '孕前体重-RBP4',\n",
       " 'SNP45*SNP46',\n",
       " 'SNP36*SNP49',\n",
       " 'SNP11*SNP15',\n",
       " 'SNP33/SNP46',\n",
       " 'TG+wbc',\n",
       " 'HDLC/wbc',\n",
       " 'TG*ALT']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目前特征长度为 1  目前帅气的cv值是 0.529126383031  成功加入第 1 个 增值为 0.529126383031\n",
      "目前特征长度为 2  目前帅气的cv值是 0.541969584999  成功加入第 2 个 增值为 0.0128432019677\n",
      "目前特征长度为 3  目前帅气的cv值是 0.568554327993  成功加入第 3 个 增值为 0.0265847429934\n",
      "目前特征长度为 4  目前帅气的cv值是 0.57652332479  成功加入第 11 个 增值为 0.0079689967979\n",
      "目前特征长度为 5  目前帅气的cv值是 0.59432805946  成功加入第 12 个 增值为 0.0178047346692\n",
      "目前特征长度为 6  目前帅气的cv值是 0.594995772882  成功加入第 14 个 增值为 0.000667713422274\n",
      "目前特征长度为 7  目前帅气的cv值是 0.601384057634  成功加入第 18 个 增值为 0.00638828475201\n",
      "目前特征长度为 8  目前帅气的cv值是 0.621135701011  成功加入第 20 个 增值为 0.0197516433766\n",
      "目前特征长度为 9  目前帅气的cv值是 0.659833147249  成功加入第 24 个 增值为 0.0386974462387\n",
      "目前特征长度为 10  目前帅气的cv值是 0.678642746028  成功加入第 25 个 增值为 0.0188095987783\n",
      "目前特征长度为 11  目前帅气的cv值是 0.685003138629  成功加入第 31 个 增值为 0.00636039260157\n",
      "目前特征长度为 12  目前帅气的cv值是 0.686918440568  成功加入第 33 个 增值为 0.00191530193904\n",
      "目前特征长度为 13  目前帅气的cv值是 0.689605039799  成功加入第 44 个 增值为 0.00268659923099\n",
      "目前特征长度为 14  目前帅气的cv值是 0.691387941235  成功加入第 53 个 增值为 0.00178290143601\n",
      "目前特征长度为 15  目前帅气的cv值是 0.699233952221  成功加入第 54 个 增值为 0.00784601098582\n",
      "目前特征长度为 16  目前帅气的cv值是 0.709950933425  成功加入第 55 个 增值为 0.0107169812039\n",
      "目前特征长度为 17  目前帅气的cv值是 0.713050765167  成功加入第 56 个 增值为 0.00309983174182\n",
      "目前特征长度为 18  目前帅气的cv值是 0.714504690354  成功加入第 62 个 增值为 0.0014539251878\n",
      "目前特征长度为 19  目前帅气的cv值是 0.715334590971  成功加入第 67 个 增值为 0.000829900616473\n",
      "目前特征长度为 20  目前帅气的cv值是 0.725797483685  成功加入第 70 个 增值为 0.0104628927137\n",
      "目前特征长度为 21  目前帅气的cv值是 0.72841012063  成功加入第 87 个 增值为 0.0026126369455\n",
      "目前特征长度为 22  目前帅气的cv值是 0.731058233319  成功加入第 92 个 增值为 0.00264811268908\n"
     ]
    }
   ],
   "source": [
    "now_feature2 = []\n",
    "check = 0\n",
    "for i in range(len(get_ans_face)):\n",
    "    now_feature2.append(get_ans_face[len(get_ans_face)-i-1])\n",
    "    jj = find_best_feature(now_feature2,6)\n",
    "    if jj>check:\n",
    "        print('目前特征长度为',len(now_feature2),' 目前帅气的cv值是',jj,' 成功加入第',i+1,'个','增值为',jj-check)\n",
    "        check = jj\n",
    "    else:\n",
    "        now_feature2.pop()\n",
    "#         print('目前特征长度为',len(now_feature),'第',i+1,'个拉闸了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proba(ans):\n",
    "    kfc = []\n",
    "    tot0 = 0\n",
    "    tot1 = 0\n",
    "    for i in range(len(ans)):\n",
    "        if ans[i][0]>0.5:\n",
    "            kfc.append(0)\n",
    "            tot0 += 1\n",
    "        else:\n",
    "            kfc.append(1)\n",
    "            tot1 += 1\n",
    "    print('1 = ',tot1,' ','0 =',tot0)\n",
    "    return kfc\n",
    "# ans1 = get_proba(train_best_feature(now_feature_1))\n",
    "ans1 = get_proba(train_best_feature(now_feature))\n",
    "# ans3 = get_proba((train_best_feature(now_feature2)+train_best_feature(now_feature))/2)\n",
    "ans2 = get_proba(train_best_feature(now_feature2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF(ans).to_csv('真的不想做了.csv',header=False,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro1 = lgb_model.predict_proba(test_data[feature_name])\n",
    "pro2 = xgb_model.predict_proba(test_data[feature_name].values)\n",
    "pro3 = gbc_model.predict_proba(test_data[feature_name].fillna(7).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_ans = lgb_model.predict(test_data[feature_name])\n",
    "xgb_ans = xgb_model.predict(test_data[feature_name].values)\n",
    "gbc_ans = gbc_model.predict(test_data[feature_name].fillna(7.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "kfc = []\n",
    "for i in range(len(lgb_ans)):\n",
    "    if (lgb_ans[i]==xgb_ans[i]):\n",
    "        kfc.append(lgb_ans[i])\n",
    "    elif (lgb_ans[i]==gbc_ans[i]):\n",
    "        kfc.append(lgb_ans[i])\n",
    "    elif (gbc_ans[i]==xgb_ans[i]):\n",
    "        kfc.append(gbc_ans[i])\n",
    "    else:\n",
    "        kfc.append(gbc_ans[i])\n",
    "        \n",
    "print(len(kfc))\n",
    "DF(kfc).to_csv('ans_fuck2.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
